Agent: SAC Agent

Model: m_0 (3814 checkpoint from the retraining)

Hyperparameters: {
    'actor_architecture': (64, 64), 
    'actor_activation_function': <class 'torch.nn.modules.activation.ReLU'>, 
    'critic_architecture': (64, 64), 
    'critic_activation_function': <class 'torch.nn.modules.activation.ReLU'>
    }

Also testing smaller gap between updates for sample efficiency

gamma: float = 0.99,
polyak: float = 0.995,
q_lr: float = 1e-3,
q_architecture: tuple = (64, 64),
q_activation_function: F = nn.ReLU,
policy_lr: float = 1e-3,
policy_architecture: tuple = (64, 64),
policy_activation_function: F = nn.ReLU,
buffer_size: int = 1_000_000,
alpha: float = 0.2,
exploration_timesteps: int = 1_000, # Less exploration => Maybe they take an average earlier than 100K?
update_frequency_in_episodes: int = 1,
update_start_in_episodes: int = 100,
number_of_batch_updates: int = 1_000,
batch_size: int = 100,

Exploring 10K gives 164 sample efficiency on the leaderboard (With same update start and frequency)
Exploring 1K gives ? sample efficiency on the leaderboard (With same update start and frequency)
